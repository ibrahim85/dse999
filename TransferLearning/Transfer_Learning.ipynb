{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "We'll review the idea of transfer learning and data augmentation.\n",
    "\n",
    "The dataset we'll use is the \"cats versus dogs\" dataset from Kaggle.  It's a collection of cat and dog photos obtained from Flickr (yes, that's what happens when you blindly accept the terms of service-- your photos become public domain).\n",
    "\n",
    "This notebook is taken from Francois Chollet's excellent [blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). Francois points out that when this dataset was first posted, the notes mentioned that a classifer that scored 60% would be very impressive and require \"a major advance in the state of the art\". That was in 2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "What happens if you don't have \"enough\" data for deep learning?  Here we have about 25,000 images to both train and validate our models. That's probably way too few examples for a convolutional network to generalize the statistics and fit good convolutional filters. However, this presents an opportunity. \n",
    "\n",
    "Ideally, we want a classifier that can recognize cat versus dog regardless of where the cat is in the picture or how the cat is rotated or scaled. It'd be less useful to have a classifier that only works well when the cat is centered and in perfect focus. (Will a cat stand still for that long?)\n",
    "\n",
    "So what if we take our original photos and rotate, translate, flip, zoom, and blur them at random? Keras has a tool called ImageDataGenerator which does just that. It uses openCV to rotate, shift, shear, and zoom photos randomly as they are fed into the model for training. So every time the model sees the picture it looks slightly different. Hence one photo can become 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Generator\n",
    "\n",
    "Here's so code that just shows the image generator in action.  We'll take a cat image (cat.0.jpg), run it through our data generator, and save 20 \"versions\" of the image-- each one is slightly different. In practice, we don't actually save these pictures, but this is just a way for us to view what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network (CNN)\n",
    "\n",
    "Here's a very simple CNN with 3 convolutional layers and 3 max pooling layers. Very, very simple. And, yet we are already scoring better accuracy than the experts from 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Now we flatten out the model and run it through one dense layer and one sigmoid function layer.\n",
    "\n",
    "So our output is a single number between 0 and 1 (probability of cat or dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23426 images belonging to 2 classes.\n",
      "Found 1574 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 12s - loss: 0.7045 - acc: 0.5290 - val_loss: 0.6704 - val_acc: 0.6225\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 11s - loss: 0.6852 - acc: 0.5715 - val_loss: 0.5816 - val_acc: 0.6980\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 11s - loss: 0.6542 - acc: 0.6375 - val_loss: 0.5840 - val_acc: 0.6763\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 11s - loss: 0.6312 - acc: 0.6545 - val_loss: 0.5713 - val_acc: 0.7206\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 11s - loss: 0.6307 - acc: 0.6675 - val_loss: 0.6066 - val_acc: 0.7238\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 11s - loss: 0.6263 - acc: 0.6635 - val_loss: 0.6591 - val_acc: 0.6930\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 11s - loss: 0.6151 - acc: 0.6890 - val_loss: 0.4885 - val_acc: 0.7638\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 11s - loss: 0.6001 - acc: 0.6940 - val_loss: 0.5938 - val_acc: 0.6805\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 11s - loss: 0.6038 - acc: 0.6975 - val_loss: 0.5362 - val_acc: 0.7250\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 11s - loss: 0.5894 - acc: 0.7110 - val_loss: 0.5255 - val_acc: 0.7231\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 11s - loss: 0.6011 - acc: 0.7150 - val_loss: 0.6242 - val_acc: 0.6587\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 11s - loss: 0.5726 - acc: 0.7185 - val_loss: 0.4893 - val_acc: 0.7581\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 11s - loss: 0.5563 - acc: 0.7305 - val_loss: 0.6288 - val_acc: 0.7137\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 11s - loss: 0.5814 - acc: 0.7125 - val_loss: 0.5133 - val_acc: 0.7494\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 11s - loss: 0.6003 - acc: 0.7210 - val_loss: 0.5393 - val_acc: 0.7500\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 11s - loss: 0.5706 - acc: 0.7410 - val_loss: 0.5156 - val_acc: 0.7594\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 11s - loss: 0.6073 - acc: 0.7040 - val_loss: 0.5108 - val_acc: 0.7750\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 11s - loss: 0.5763 - acc: 0.7245 - val_loss: 0.6524 - val_acc: 0.7043\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 11s - loss: 0.5609 - acc: 0.7315 - val_loss: 0.4541 - val_acc: 0.7825\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 11s - loss: 0.5646 - acc: 0.7435 - val_loss: 0.6049 - val_acc: 0.7318\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 11s - loss: 0.5996 - acc: 0.7050 - val_loss: 0.5419 - val_acc: 0.7325\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 11s - loss: 0.5766 - acc: 0.7365 - val_loss: 0.4725 - val_acc: 0.7757\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 11s - loss: 0.5618 - acc: 0.7340 - val_loss: 0.4459 - val_acc: 0.7950\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 11s - loss: 0.5320 - acc: 0.7400 - val_loss: 0.4895 - val_acc: 0.7757\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 11s - loss: 0.5289 - acc: 0.7550 - val_loss: 0.5384 - val_acc: 0.7238\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 11s - loss: 0.5715 - acc: 0.7345 - val_loss: 0.5024 - val_acc: 0.7644\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 11s - loss: 0.5384 - acc: 0.7420 - val_loss: 0.4323 - val_acc: 0.8100\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 11s - loss: 0.5282 - acc: 0.7470 - val_loss: 0.4982 - val_acc: 0.7481\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 11s - loss: 0.5539 - acc: 0.7450 - val_loss: 0.7194 - val_acc: 0.6637\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 11s - loss: 0.5551 - acc: 0.7450 - val_loss: 0.3899 - val_acc: 0.8333\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 11s - loss: 0.5536 - acc: 0.7420 - val_loss: 0.6955 - val_acc: 0.6412\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 11s - loss: 0.5432 - acc: 0.7465 - val_loss: 0.3876 - val_acc: 0.8246\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 11s - loss: 0.5601 - acc: 0.7420 - val_loss: 0.4784 - val_acc: 0.7800\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 11s - loss: 0.5412 - acc: 0.7580 - val_loss: 0.5558 - val_acc: 0.7093\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 11s - loss: 0.5446 - acc: 0.7475 - val_loss: 0.4862 - val_acc: 0.7800\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 11s - loss: 0.5482 - acc: 0.7605 - val_loss: 0.4035 - val_acc: 0.8258\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 11s - loss: 0.5284 - acc: 0.7540 - val_loss: 0.5358 - val_acc: 0.7318\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 11s - loss: 0.5346 - acc: 0.7630 - val_loss: 0.5826 - val_acc: 0.7175\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 11s - loss: 0.5526 - acc: 0.7555 - val_loss: 0.4701 - val_acc: 0.7807\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 11s - loss: 0.5489 - acc: 0.7420 - val_loss: 0.5048 - val_acc: 0.7488\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 11s - loss: 0.5329 - acc: 0.7575 - val_loss: 0.4525 - val_acc: 0.8145\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 11s - loss: 0.5189 - acc: 0.7695 - val_loss: 0.4143 - val_acc: 0.8050\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 11s - loss: 0.5326 - acc: 0.7675 - val_loss: 0.4818 - val_acc: 0.7757\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 11s - loss: 0.5334 - acc: 0.7485 - val_loss: 0.4332 - val_acc: 0.8087\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 11s - loss: 0.5579 - acc: 0.7355 - val_loss: 0.4113 - val_acc: 0.8258\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 11s - loss: 0.5364 - acc: 0.7590 - val_loss: 0.4410 - val_acc: 0.8013\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 11s - loss: 0.5223 - acc: 0.7730 - val_loss: 0.3772 - val_acc: 0.8246\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 11s - loss: 0.5411 - acc: 0.7565 - val_loss: 0.4252 - val_acc: 0.8087\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 11s - loss: 0.5581 - acc: 0.7500 - val_loss: 0.4511 - val_acc: 0.8058\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 11s - loss: 0.5196 - acc: 0.7670 - val_loss: 0.4663 - val_acc: 0.7887\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45251897763895521, 0.78818227343996228]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78% accuracy without breaking a sweat\n",
    "\n",
    "You now have a model that would have won you fame and fortune in 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pre-trained VGG16\n",
    "\n",
    "Let's see if we can do even better by transfering knowledge from one model into another.\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman at the University of Oxford's [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) created the VGG convolutional model in 2012 and scored first (localization task) and second (classification task) place in the ImageNet 2014 challenge. \n",
    "\n",
    "They have two [VGG models](http://arxiv.org/pdf/1409.1556):\n",
    "+ VGG16 - a 16-layer CNN\n",
    "![VGG16](http://book.paddlepaddle.org/03.image_classification/image/vgg16.png)\n",
    "+ VGG19 - a 19-layer CNN\n",
    "![VGG19](https://image.slidesharecdn.com/neuralarten-160316180514/95/neural-art-english-version-19-638.jpg?cb=1458367899)\n",
    "\n",
    "Both are released under the Creative Commons License and have been used as starting points for many researchers to create new neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = 'vgg16_weights.h5'\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16\n",
    "\n",
    "Because VGG has an open-source license, it is included in many packages (including Keras).  Note that the weights file is over 500 MB in size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append our new layers\n",
    "\n",
    "VGG was used to predict one of 1,000 classes. We only want to predict either cat (0) or dog (1). To do this, we'll just append a few more dense layers to the network and add a single output with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))  # Sigmoid goes between 0 and 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the two parts\n",
    "\n",
    "Here's where we add VGG16 to our custom output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the model on top of the VGG base\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't train VGG again\n",
    "\n",
    "The point of using VGG is that it is already trained. Yes, it was trained on a completely different set of images, but the basic concept is that the convolutional filters \"might\" be applicable to any real image. \n",
    "\n",
    "So here we make sure that we *do not* change the weights of the first 16 layers of our new network.  We instead will only train our additional layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23426 images belonging to 2 classes.\n",
      "Found 1574 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 2097665   \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48s - loss: 0.6135 - acc: 0.6640 - val_loss: 0.4372 - val_acc: 0.8250\n",
      "Epoch 2/50\n",
      "47s - loss: 0.4326 - acc: 0.8060 - val_loss: 0.3037 - val_acc: 0.8734\n",
      "Epoch 3/50\n",
      "45s - loss: 0.3519 - acc: 0.8500 - val_loss: 0.2572 - val_acc: 0.8788\n",
      "Epoch 4/50\n",
      "45s - loss: 0.3093 - acc: 0.8685 - val_loss: 0.2452 - val_acc: 0.8899\n",
      "Epoch 5/50\n",
      "45s - loss: 0.3168 - acc: 0.8605 - val_loss: 0.2698 - val_acc: 0.8875\n",
      "Epoch 6/50\n",
      "45s - loss: 0.2900 - acc: 0.8695 - val_loss: 0.2211 - val_acc: 0.9062\n",
      "Epoch 7/50\n",
      "45s - loss: 0.2660 - acc: 0.8905 - val_loss: 0.2609 - val_acc: 0.8861\n",
      "Epoch 8/50\n",
      "46s - loss: 0.2771 - acc: 0.8875 - val_loss: 0.2019 - val_acc: 0.9213\n",
      "Epoch 9/50\n",
      "45s - loss: 0.2480 - acc: 0.8975 - val_loss: 0.2099 - val_acc: 0.9127\n",
      "Epoch 10/50\n",
      "45s - loss: 0.2380 - acc: 0.9020 - val_loss: 0.2050 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      "45s - loss: 0.2337 - acc: 0.9015 - val_loss: 0.2085 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "47s - loss: 0.2161 - acc: 0.9110 - val_loss: 0.1854 - val_acc: 0.9190\n",
      "Epoch 13/50\n",
      "45s - loss: 0.2216 - acc: 0.9065 - val_loss: 0.1744 - val_acc: 0.9263\n",
      "Epoch 14/50\n",
      "45s - loss: 0.2215 - acc: 0.9040 - val_loss: 0.1691 - val_acc: 0.9190\n",
      "Epoch 15/50\n",
      "45s - loss: 0.2250 - acc: 0.9125 - val_loss: 0.1995 - val_acc: 0.9250\n",
      "Epoch 16/50\n",
      "45s - loss: 0.2056 - acc: 0.9190 - val_loss: 0.1759 - val_acc: 0.9375\n",
      "Epoch 17/50\n",
      "45s - loss: 0.2103 - acc: 0.9070 - val_loss: 0.1819 - val_acc: 0.9228\n",
      "Epoch 18/50\n",
      "45s - loss: 0.1937 - acc: 0.9195 - val_loss: 0.1768 - val_acc: 0.9263\n",
      "Epoch 19/50\n",
      "45s - loss: 0.1839 - acc: 0.9280 - val_loss: 0.2242 - val_acc: 0.9063\n",
      "Epoch 20/50\n",
      "45s - loss: 0.1950 - acc: 0.9185 - val_loss: 0.1674 - val_acc: 0.9342\n",
      "Epoch 21/50\n",
      "45s - loss: 0.2061 - acc: 0.9235 - val_loss: 0.1574 - val_acc: 0.9425\n",
      "Epoch 22/50\n",
      "45s - loss: 0.1854 - acc: 0.9190 - val_loss: 0.1682 - val_acc: 0.9342\n",
      "Epoch 23/50\n",
      "45s - loss: 0.1616 - acc: 0.9370 - val_loss: 0.1834 - val_acc: 0.9263\n",
      "Epoch 24/50\n",
      "45s - loss: 0.1697 - acc: 0.9435 - val_loss: 0.1892 - val_acc: 0.9215\n",
      "Epoch 25/50\n",
      "45s - loss: 0.1720 - acc: 0.9295 - val_loss: 0.1715 - val_acc: 0.9266\n",
      "Epoch 26/50\n",
      "45s - loss: 0.1923 - acc: 0.9270 - val_loss: 0.1823 - val_acc: 0.9287\n",
      "Epoch 27/50\n",
      "45s - loss: 0.1742 - acc: 0.9240 - val_loss: 0.1790 - val_acc: 0.9342\n",
      "Epoch 28/50\n",
      "45s - loss: 0.1644 - acc: 0.9360 - val_loss: 0.1326 - val_acc: 0.9363\n",
      "Epoch 29/50\n",
      "45s - loss: 0.1718 - acc: 0.9325 - val_loss: 0.1702 - val_acc: 0.9266\n",
      "Epoch 30/50\n",
      "45s - loss: 0.1563 - acc: 0.9355 - val_loss: 0.1308 - val_acc: 0.9481\n",
      "Epoch 31/50\n",
      "45s - loss: 0.1648 - acc: 0.9315 - val_loss: 0.1482 - val_acc: 0.9325\n",
      "Epoch 32/50\n",
      "45s - loss: 0.1773 - acc: 0.9290 - val_loss: 0.1525 - val_acc: 0.9304\n",
      "Epoch 33/50\n",
      "45s - loss: 0.1637 - acc: 0.9330 - val_loss: 0.1553 - val_acc: 0.9463\n",
      "Epoch 34/50\n",
      "45s - loss: 0.1614 - acc: 0.9295 - val_loss: 0.1715 - val_acc: 0.9263\n",
      "Epoch 35/50\n",
      "45s - loss: 0.1750 - acc: 0.9290 - val_loss: 0.1530 - val_acc: 0.9443\n",
      "Epoch 36/50\n",
      "45s - loss: 0.1568 - acc: 0.9350 - val_loss: 0.1533 - val_acc: 0.9325\n",
      "Epoch 37/50\n",
      "45s - loss: 0.1581 - acc: 0.9325 - val_loss: 0.1567 - val_acc: 0.9418\n",
      "Epoch 38/50\n",
      "45s - loss: 0.1629 - acc: 0.9425 - val_loss: 0.1266 - val_acc: 0.9463\n",
      "Epoch 39/50\n",
      "45s - loss: 0.1521 - acc: 0.9340 - val_loss: 0.1834 - val_acc: 0.9213\n",
      "Epoch 40/50\n",
      "45s - loss: 0.1484 - acc: 0.9385 - val_loss: 0.1524 - val_acc: 0.9380\n",
      "Epoch 41/50\n",
      "45s - loss: 0.1613 - acc: 0.9375 - val_loss: 0.1339 - val_acc: 0.9450\n",
      "Epoch 42/50\n",
      "45s - loss: 0.1512 - acc: 0.9310 - val_loss: 0.1670 - val_acc: 0.9354\n",
      "Epoch 43/50\n",
      "45s - loss: 0.1650 - acc: 0.9295 - val_loss: 0.1668 - val_acc: 0.9325\n",
      "Epoch 44/50\n",
      "45s - loss: 0.1354 - acc: 0.9475 - val_loss: 0.1366 - val_acc: 0.9437\n",
      "Epoch 45/50\n",
      "45s - loss: 0.1413 - acc: 0.9450 - val_loss: 0.1556 - val_acc: 0.9430\n",
      "Epoch 46/50\n",
      "45s - loss: 0.1452 - acc: 0.9400 - val_loss: 0.1914 - val_acc: 0.9150\n",
      "Epoch 47/50\n",
      "45s - loss: 0.1482 - acc: 0.9410 - val_loss: 0.1718 - val_acc: 0.9304\n",
      "Epoch 48/50\n",
      "45s - loss: 0.1525 - acc: 0.9410 - val_loss: 0.1452 - val_acc: 0.9437\n",
      "Epoch 49/50\n",
      "45s - loss: 0.1330 - acc: 0.9460 - val_loss: 0.1136 - val_acc: 0.9500\n",
      "Epoch 50/50\n",
      "45s - loss: 0.1315 - acc: 0.9460 - val_loss: 0.1586 - val_acc: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd31920e810>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14301167962855935, 0.94245283023366389]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greater than 90% accuracy in under 40 minutes!\n",
    "\n",
    "That's definitely not hype. Plus, we have a model that can take pictures of cats and dogs from any angle, any scale, and/or any translation and not be confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
