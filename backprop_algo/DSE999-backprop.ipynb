{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackProp some computational intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "seed(10)\n",
    "from random import random\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a simple network with random weights which will provide the structure to forward propagate, produce a y-hat and backpropagate an error term to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple function to produce a list of random weights from 0-1\n",
    "def gen_weights(num_weights):\n",
    "    bias = 1 #for the case where weight is 0, as the bias is never multiplied inside the network\n",
    "    weights = []\n",
    "    for n in range(num_weights + bias):\n",
    "        weights.append(random()) #add a random weight\n",
    "    return weights\n",
    "        \n",
    "\n",
    "# Spin up a network w/ random weights. This network is represented layer by layer as arrays. \n",
    "def create_neural_network(inputs, layers, outputs):\n",
    "    neural_network = []\n",
    "    hidden_layer = []\n",
    "    output_layer = []\n",
    "    \n",
    "    # Generate the weights for the layers randomly\n",
    "    for n in range(layers):\n",
    "        hidden_layer.append({'weight' : gen_weights(inputs)})\n",
    "        \n",
    "    for n in range(outputs):\n",
    "        output_layer.append({'weight' : gen_weights(outputs)}) # +1 for the bias term\n",
    "    \n",
    "    neural_network.append(hidden_layer) #add the hidden layers to the network\n",
    "    neural_network.append(output_layer) #add the output layer to the netowrk\n",
    "    \n",
    "    return neural_network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons in hidden layer: 1\n",
      "Number of neurons in output layer: 2\n",
      "[{'weight': [0.2823220462034699, 0.186365561491604, 0.16853039195927333]}]\n",
      "[{'weight': [0.09545322921951538, 0.790572738775103, 0.9357357476153892]}, {'weight': [0.21987633831820674, 0.744912691964874, 0.9511437223975017]}]\n"
     ]
    }
   ],
   "source": [
    "network = create_neural_network(2, 1, 2)\n",
    "print 'Number of neurons in hidden layer: {}' .format(len(network[0]))\n",
    "print 'Number of neurons in output layer: {}' .format(len(network[1]))\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to do is create a feed forward algorithm that will return an ouput given our network's weights and it will do this via an activation function that we prescribe. We shall choose a sigmoid function to determine if the neuron gets 'excited' enough to fire and trasmit information along its 'axon'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic sigmoid function looks like: $f(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activator_function(neuron_weight, input_feed):\n",
    "#     print neuron_weight\n",
    "    activation = neuron_weight['weight'][-1]\n",
    "#     print activation\n",
    "    for w in range(len(neuron_weight)-1): #-1 for bias as we only want to update the neurons for which we have input\n",
    "        activation += neuron_weight[w] * input_feed[w]\n",
    "    return activation\n",
    "\n",
    "\n",
    "def feed_forward_prop(neural_network, inputs):\n",
    "    \n",
    "    for layer in neural_network:\n",
    "        #print layer\n",
    "        updated_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activator_function(neuron, inputs)\n",
    "            print type(neuron)\n",
    "            print activation\n",
    "            #activate, transfer input, append new input\n",
    "#     return updated_inputes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "0.168530391959\n",
      "<type 'dict'>\n",
      "0.935735747615\n",
      "<type 'dict'>\n",
      "0.951143722398\n"
     ]
    }
   ],
   "source": [
    "network_output = feed_forward_prop(network, [0,1])\n",
    "network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output gives us a value we can calculate the derivative of, which we will use to \n",
    "# propegate backwards to update the weights.\n",
    "\n",
    "der = network_output * (1 - float(output))\n",
    "\n",
    "# we can use this derivative to get the error, e.\n",
    "# y is the neuron's true correct output correct, yhat is the actual prediction, der is the slope (think gradient)\n",
    "e = (y - yhat)  * der "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our error we can use it to update the weights in the network, before we feed forward again. We want to write a function that goes through the netowrk we have and updates the weights based on an error and gradient that we calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# toy data\n",
    "# data = [\n",
    "#         [2.7810836,2.3446661,0],\n",
    "#         [3.14324,2.1355523,0],\n",
    "#         [2.23435,1.14335115,0],\n",
    "#         [10.627531214,2.143515,1],\n",
    "#         [8.332441248,2.43511,1],\n",
    "#         [9.875114,1.77324,1]\n",
    "#        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
