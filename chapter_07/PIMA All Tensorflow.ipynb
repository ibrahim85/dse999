{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIMA MLP in TensorFlow\n",
    "\n",
    "This is the same 12-8-1 MLP model that was shown in Keras. Now I am using just TensorFlow operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The beginning is the same as with Keras. Just loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the TensorFlow examples use multi-class even when there is just one class\n",
    "\n",
    "I'm sure there is a way to use the sigmoid function and just predict one class, but wasn't readily finding it. All of the examples seem to use softmax multi-class predictors.\n",
    "\n",
    "I kept this format since we will often use multi-class and since this shows the concept of [OneHot Encoding](https://en.wikipedia.org/wiki/One-hot).  \n",
    "\n",
    "\n",
    "To one hot encode, we simply have take each possible value and turn it into its own category. So for a \"diabetes\"/\"no diabetes\" binary classifier, we need 2 bits (01 and 10). This means that we'll have 2 y values for each possible outcome: 01 for no diabetes and 10 for diabetes.\n",
    "\n",
    "Sample # | Category | One Hot Encode\n",
    ":-----: | ------| ------\n",
    "1 | Diabetes | 1,0\n",
    "2 | No Diabetes | 0,1\n",
    "3 | Diabetes | 1,0\n",
    "4 | Diabetes | 1,0\n",
    "5 | Diabetes | 1,0\n",
    "6 | No diabetes | 0,1\n",
    "7 | No diabetes | 0,1\n",
    "\n",
    "So below, we are turning our original one-column label of Diabetes (0,1), into two columns: the first column is if the person has diabetes, the second is if they don't. Note that this means 1,1 and 0,0 are meaningless for one-hot in this case.\n",
    "\n",
    "Our softmax is going to give a prediction (probability) of which class we are in. So it will say something like: no diabetes = 0.33 and diabetes = 0.66. So we just need to find the argmax to get the prediction (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([y, 1-y]).T  # One hot encode the binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we start the Tensorflow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is how we setup our learning rate ($\\alpha$) for backprop\n",
    "\n",
    "We start at rate 0.01 and then with each step (global_step=1), we exponentially decrease $\\alpha$. This is along the same lines as an algorithm called [simulated annealing](http://katrinaeg.com/simulated-annealing.html). We start taking large steps (large $\\alpha$) in our random walk and then take progressively shorter steps as we feel we are approaching the minimum value. In true simulated annealing, we are actually calculating the probability of taking a step in the new direction (based on how good the new direction seems to be and how close we think we are to the bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.train.exponential_decay(learning_rate=0.01,\n",
    "                                           global_step=1,\n",
    "                                           decay_steps=X_train.shape[0],\n",
    "                                           decay_rate=0.95,\n",
    "                                           staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 150   # Let's go through the training step 150 times (maximum)\n",
    "batch_size = 10        # Let's consider 10 training examples at a time (so 150/10 = 15 iterations per epoch)\n",
    "display_step = 10       # Let's print out the error for every 10 training examples (i.e. every batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the hyperparameters.\n",
    "\n",
    "We need to define these ahead of time. Basically, # of layers and number of neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1 = 12                   # 12 neurons in first layer\n",
    "n_hidden_2 = 8                    # 8 neurons in second layer\n",
    "n_hidden_3 = 1                    # 1 neuron in the third layer\n",
    "n_input_features = np.shape(X_train)[1]    # Our input layer depends on shape of data features\n",
    "n_classes = np.shape(y_train)[1]  # Our output layer depends on shape of data label\n",
    "dropout = 1.0-0.2             # Let's add dropout to our model (20% of the neurons will dropout randomly for each pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember TensorFlow Placeholders?\n",
    "\n",
    "These are variables that will expect a stream of external data. We use these for our X and y variables.\n",
    "\n",
    "Note that with x, we do not assume how many rows of data we have. Instead we just tell it [None]. So the Tensorflow graph just knows it will receive an unknown number of n_input vectors (i.e. out features per training example). This is really nice because the algorithm is set to handle 1 to 900 zillion examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input_features])  \n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's the neural network\n",
    "\n",
    "In Keras we did this:\n",
    "\n",
    "~~~~\n",
    "model = Sequential() \n",
    "model.add(Dense(12, input_dim=8, activation='relu')) # Layer 1\n",
    "model.add(Dense(6, activation='relu'))               # Layer 2\n",
    "model.add(Dropout(0.5))                              # Dropout\n",
    "model.add(Dense(1, activation='sigmoid'))            # Output layer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x, weights, biases):\n",
    "    \n",
    "   # Hidden layer with relu activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Hidden layer with relu activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    layer_2 = tf.nn.dropout(layer_2, dropout)\n",
    "    \n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    \n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    " \n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Keras we didn't have to explicitly set the initial values of the weights.\n",
    "\n",
    "(Although we could). There are lots of algorithms for setting the initial values. Here we are simple setting them based on a uniform distribution from 0.1 to 0.4\n",
    "\n",
    "Note that here we use TensorFlow **Variable** type. We don't use placeholders because these values are all internal to the graph. We'll update them just like any other internal variable. Also, note that TF uses LazyExecution. So these Variables are not actually allocated or assigned until explicitly told to do so (by running the session). We'll use teh Tensorflow command tf.global_variables_initializer() to initialize all the variables to their default values at the start of the graph execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "minval = 0.1\n",
    "maxval = 0.4\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform(shape=(n_input_features,n_hidden_1),minval=minval, maxval=maxval, dtype=tf.float32, seed=0)),\n",
    "    'h2': tf.Variable(tf.random_uniform(shape=(n_hidden_1, n_hidden_2),minval=minval, maxval=maxval, dtype=tf.float32, seed=0)),\n",
    "    'h3': tf.Variable(tf.random_uniform(shape=(n_hidden_2, n_hidden_3),minval=minval, maxval=maxval, dtype=tf.float32, seed=0)),\n",
    "    'out': tf.Variable(tf.random_uniform(shape=(n_hidden_3, n_classes),minval=minval, maxval=maxval, dtype=tf.float32, seed=0))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_uniform([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_uniform([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_uniform([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Op\n",
    "\n",
    "Now we assign a TensorFlow operation (Op) called $pred$ which contains our custom function $neural\\_network$.  The key here is that $neural\\_network$ can be copied to multiple GPU/CPUs and run in parallel with our data. No training example is dependent on any other.\n",
    "\n",
    "As with Spark, TensorFlow lazy execution won't actually run the $pred$ op graph until explicitly needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost is yet another Op in our graph.\n",
    "\n",
    "Here we create another operation called \"cost\" which tells us how close our MLP's prediction ($pred$) is to the true value ($y$). The equation is:\n",
    "\n",
    "$$-y log(pred) - (1 - y) log(1 - pred)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now our gradient descent Op\n",
    "\n",
    "We'll use [Adam](https://arxiv.org/abs/1412.6980) as our gradient descent/backpropagation algorithm. Adam uses *momentum* which is a measure of how much our error is changing over the last few iterations. Adam uses a larger effective step size and will automatically converge to the best step size without the need for hand tuning. So it tends to be good at handling some of the hyperparameters. In short, Adam gets you to the bottom of the hill without needing to hand tune your learning rate ($alpha$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now our initialize variables Op\n",
    "\n",
    "This sets all of our TF variables (e.g. weights, learning rates) to their default values. If we don't do this, then TensorFlow will throw a runtime error because the variables haven't been initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run the TF graph in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\t cost = 5.803\n",
      "epoch 11:\t cost = 0.630\n",
      "epoch 21:\t cost = 0.573\n",
      "epoch 31:\t cost = 0.571\n",
      "epoch 41:\t cost = 0.566\n",
      "epoch 51:\t cost = 0.550\n",
      "epoch 61:\t cost = 0.556\n",
      "epoch 71:\t cost = 0.544\n",
      "epoch 81:\t cost = 0.532\n",
      "epoch 91:\t cost = 0.508\n",
      "epoch 101:\t cost = 0.520\n",
      "epoch 111:\t cost = 0.522\n",
      "epoch 121:\t cost = 0.520\n",
      "epoch 131:\t cost = 0.508\n",
      "epoch 141:\t cost = 0.485\n",
      "\n",
      "Accuracy on training set = 0.7866\n",
      "Accuracy on test set = 0.7597\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)   # First run the variable initializing Op\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        \n",
    "        total_batch = int(len(X_train) / batch_size)  # Calculate the batch size\n",
    "\n",
    "        X_batches = np.array_split(X_train, total_batch)  # Grab a batch\n",
    "        Y_batches = np.array_split(y_train, total_batch)  # Grab a batch\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]  # Here's how we feed the PlaceHolder\n",
    "\n",
    "            # We run both the optimizer and cost Op nodes of the graph\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"epoch {}:\\t cost = {:.3f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    \n",
    "    # Now let's use our model to get the accuracy for the test set prediction\n",
    "    # This is another Op on our graph. We test whether 'pred' and 'y' are equal.\n",
    "    \n",
    "    # We one-hot encoded so the prediction will be determined by which of the two bits is larger\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "    # Calculate accuracy - Yet another Op\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    # Now execute the 'accuracy' Op (which calls the 'correct_prediction' op which calls the 'model op')\n",
    "    print(\"Accuracy on training set = {:.4f}\".format(accuracy.eval({x: X_train, y: y_train})))\n",
    "    \n",
    "    # Now execute the 'accuracy' Op (which calls the 'correct_prediction' op which calls the 'model op')\n",
    "    print(\"Accuracy on test set = {:.4f}\".format(accuracy.eval({x: X_test, y: y_test})))\n",
    "    \n",
    "    predTest = sess.run(tf.argmax(pred, 1), feed_dict={x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
