{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Diabetes Multi-layered Perceptron (MLP) Model\n",
    "\n",
    "From Bogardus and Lilioja, _Pima Indians as a model to study the genetics of NIDDM_. J Cell Biochem. 1992 Apr;48(4):337-43.\n",
    "\n",
    "> More than half the Pima Indians over 35 years of age have non-insulin dependent diabetes mellitus (NIDDM). They have been the focus of prospective epidemiologic and metabolic studies for over two decades and the data collected during these studies are now proving invaluable in efforts to find genetic markers for NIDDM in humans. The Pima Indian model of this disease affords two major advantages. The population is genetically homogeneous compared to Caucasian populations, and therefore the causes of NIDDM are less heterogeneous, simplifying genetic linkage studies. Equally important, based on results from metabolic studies, two pre-diabetic phenotypes have been identified in the Pimas: insulin resistance and a low metabolic rate. Use of these phenotypes in genetic linkage analyses should greatly improve chances of finding genetic markers for NIDDM since these phenotypes may be more closely related to the putative abnormal gene products, and actual disease genes, than is the hyperglycemia of the fully developed phenotype of NIDDM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this dataset has physiological values for human subjects and is trying to see if those values can be used to predict if someone has diabetes or not. This is a classic binary classification problem.\n",
    "\n",
    "According to the [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database) website these are the features:\n",
    "\n",
    "Feature name | Feature description\n",
    "-------------- | ------------------\n",
    "Pregnancies | Number of times pregnant\n",
    "Glucose | Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "Blood Pressure | Diastolic blood pressure (mm Hg)\n",
    "Skin Thickness | Triceps skin fold thickness (mm)\n",
    "Insulin | 2-Hour serum insulin (mu U/ml)\n",
    "BMI | Body mass index (weight in kg/(height in m)^2)\n",
    "Diabetes Pedigree Function | Diabetes pedigree function\n",
    "Age| Age (years)\n",
    "\n",
    "\n",
    "Outcome: Class variable (0=no diabetes or 1=diabetes)\n",
    "\n",
    "Kaggle says that the publically-available data we have is a subset of all woman over the age of 21.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level neural network API written in Python. It capable of running on top of either TensorFlow, CNTK or Theano. It allows fast experimentation with deep neural network architectures because it handles a lot of the legwork in coding. Think of it as an abstraction of TensorFlow that is specific for generating convolutional and recurrent neural networks.\n",
    "\n",
    "Keras will automatically determine which deep learning library you have installed. It will default to TensorFlow but fall back on Theano if TensorFlow is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the Keras code to set up a multi-layered neural network. \n",
    "\n",
    "The code here is generic. If you are using Theano, this will generate Theano code under the hood. If you are using TensorFlow, then it will generate TensorFlow code under the hood. Same high-level Keras code for two different libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential layers\n",
    "\n",
    "The simplest neural network is just a linear stack of layers. So layer A goes into layer B which goes into layer C, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()  # My model is a series of connected neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer\n",
    "\n",
    "We add our first layer: It's a \"dense\" layer with 12 nodes and expects 8 inputs (from the X data). So this is 12 neurons. Each neuron gets all 8 inputs. The activation function in a rectified linear unit (ReLu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(12, input_dim=8, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second layer\n",
    "\n",
    "Our second layer is another \"dense\" layer. We don't need to specify the number of inputs to the layer. Keras already knows that it expects to receive everything output from the previous layer (12 inputs). This layer has 6 nodes and again uses ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(6, activation='relu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout layer\n",
    "\n",
    "[Dropout](https://www.youtube.com/watch?v=UcKPdAM8cnI&) is a remarkably simple idea to prevent the neural network from over-fitting. With dropout, a certain percentage of the neurons in the layer are randomly excluded from the neural network during a given training run. This seems to be enough to prevent over-fitting and it also tends to find network architectures that are optimal. Think of it like an ensemble method like boosting. Every training run is actually trying to fit a slightly different neural network.\n",
    "\n",
    "[Hinton](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf) describes it like this:\n",
    "\n",
    "> It  prevents  overfitting  and provides  a  way  of  approximately  combining  exponentially  many  different  neural  network architectures  efficiently.   The  term  “dropout”  refers  to  dropping  out  units  (hidden  and visible) in a neural network.  By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure 1. The choice of which units to drop is random.  In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set  or  can  simply  be  set  at  0.5,  which  seems  to  be  close  to  optimal  for  a  wide  range  of networks and tasks.  For the input units, however, the optimal probability of retention isusually closer to 1 than to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Layer\n",
    "\n",
    "The third layer is again a \"dense\" layer. This is our output layer. So we only need 1 neuron (1 node). Since this is a binary classifier, we use the signmoid activation function. If this were a multi-class classifier, then we would use \"softmax\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the graph\n",
    "\n",
    "Now we ask Keras to compile our graph. Since it is using TensorFlow or Theano, the backpropagation and other baggage is automatically handled for us. We'll use \"binary_crossentropy\" as the loss function (it measures how \"wrong\" our classifier is). It uses [\"adam\"](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam) for the gradient descent method (backpropagation with alpha momentum), and prints the accuracy of the model after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "This is where the actual training and calculations occur.\n",
    "\n",
    "If you have your GPU enabled, you can run 'nvidia-smi' from the command line to verify that your GPUs are being used by TensorFlow.\n",
    "\n",
    "Each epoch is one run through the entire data set. Remember that with stochasitic gradient descent, we typically need multiple runs through the data in order to get to loss minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "We can add lots of goodies when running our models. \n",
    "\n",
    "First, we should probably add an \"Early Stopping\" command. This allows the model to stop the gradient descent if it determines that the error is not changing much. We'll use \"value loss\" as our metric and wait until it hasn't improved for 10 iterations.\n",
    "\n",
    "Second, let's save the weights of our model. This way if the computer crashes, we at least have the last best values to start with when we re-run the gradient descent.\n",
    "\n",
    "Finally, we'll pass in the validation data. Keras will use this to do a simple cross-validation (training error versus testing error). I don't know if there's a K-folds. It might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/150\n",
      "Epoch 00000: val_loss improved from inf to 0.95324, saving model to pima.wts.h5\n",
      "0s - loss: 2.8912 - acc: 0.5049 - val_loss: 0.9532 - val_acc: 0.6429\n",
      "Epoch 2/150\n",
      "Epoch 00001: val_loss improved from 0.95324 to 0.80961, saving model to pima.wts.h5\n",
      "0s - loss: 0.8345 - acc: 0.6303 - val_loss: 0.8096 - val_acc: 0.6104\n",
      "Epoch 3/150\n",
      "Epoch 00002: val_loss improved from 0.80961 to 0.74161, saving model to pima.wts.h5\n",
      "0s - loss: 0.7309 - acc: 0.6515 - val_loss: 0.7416 - val_acc: 0.6364\n",
      "Epoch 4/150\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.6900 - acc: 0.6580 - val_loss: 0.8108 - val_acc: 0.6494\n",
      "Epoch 5/150\n",
      "Epoch 00004: val_loss improved from 0.74161 to 0.70128, saving model to pima.wts.h5\n",
      "0s - loss: 0.6579 - acc: 0.6547 - val_loss: 0.7013 - val_acc: 0.6429\n",
      "Epoch 6/150\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.6267 - acc: 0.6840 - val_loss: 0.7282 - val_acc: 0.6429\n",
      "Epoch 7/150\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.6244 - acc: 0.6922 - val_loss: 0.7364 - val_acc: 0.5909\n",
      "Epoch 8/150\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.6173 - acc: 0.6954 - val_loss: 0.7530 - val_acc: 0.6169\n",
      "Epoch 9/150\n",
      "Epoch 00008: val_loss improved from 0.70128 to 0.70031, saving model to pima.wts.h5\n",
      "0s - loss: 0.6194 - acc: 0.6710 - val_loss: 0.7003 - val_acc: 0.6494\n",
      "Epoch 10/150\n",
      "Epoch 00009: val_loss improved from 0.70031 to 0.69879, saving model to pima.wts.h5\n",
      "0s - loss: 0.6143 - acc: 0.6906 - val_loss: 0.6988 - val_acc: 0.6558\n",
      "Epoch 11/150\n",
      "Epoch 00010: val_loss improved from 0.69879 to 0.68358, saving model to pima.wts.h5\n",
      "0s - loss: 0.6317 - acc: 0.6906 - val_loss: 0.6836 - val_acc: 0.7273\n",
      "Epoch 12/150\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5855 - acc: 0.7117 - val_loss: 0.7295 - val_acc: 0.6883\n",
      "Epoch 13/150\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5874 - acc: 0.7248 - val_loss: 0.7039 - val_acc: 0.6494\n",
      "Epoch 14/150\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5915 - acc: 0.6922 - val_loss: 0.6934 - val_acc: 0.6818\n",
      "Epoch 15/150\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.6264 - acc: 0.6971 - val_loss: 0.7068 - val_acc: 0.7078\n",
      "Epoch 16/150\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5865 - acc: 0.6938 - val_loss: 0.7018 - val_acc: 0.7208\n",
      "Epoch 17/150\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5870 - acc: 0.7134 - val_loss: 0.6917 - val_acc: 0.6883\n",
      "Epoch 18/150\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5700 - acc: 0.7117 - val_loss: 0.7048 - val_acc: 0.6623\n",
      "Epoch 19/150\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5804 - acc: 0.7085 - val_loss: 0.6923 - val_acc: 0.6818\n",
      "Epoch 20/150\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5763 - acc: 0.6954 - val_loss: 0.6932 - val_acc: 0.6623\n",
      "Epoch 21/150\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5802 - acc: 0.7085 - val_loss: 0.7223 - val_acc: 0.6688\n",
      "Epoch 22/150\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5645 - acc: 0.7313 - val_loss: 0.7033 - val_acc: 0.7013\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Check if there is a previous weights file and load it if it exists.\n",
    "# This will allows us to resume from where we left off.\n",
    "# import os\n",
    "\n",
    "# if (os.path.exists('pima.wts.h5')):\n",
    "#     model.load_weights('pima.wts.h5')\n",
    "\n",
    "# You can also skip SkLearn and do the train/validation split with Keras natively.\n",
    "#model.fit(X, y, validation_split=0.33, epochs=150, batch_size=10)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train,\n",
    "    y_train,\n",
    "    batch_size=10,  # This is mini-batch gradient descent. So calculate 10 points at a time.\n",
    "    epochs=150,     # Maximum of 150 epochs\n",
    "    verbose=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = [\n",
    "        ModelCheckpoint('pima.wts.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of additional commands when compiling and running the model. For instance, you can do cross-validation and plot the training and testing set loss. You can also calculate F1, AUC, precision, recall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/154 [=====>........................] - ETA: 0s()\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.72      0.76        99\n",
      "        1.0       0.57      0.67      0.62        55\n",
      "\n",
      "avg / total       0.72      0.70      0.71       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_predict = model.predict_classes(X_test)\n",
    "print()\n",
    "print()\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the accuracy of the model on the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/614 [>.............................] - ETA: 0s\n",
      "acc: 71.82%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/154 [=====>........................] - ETA: 0s\n",
      "acc: 70.13%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot the training and testing errors over epochs\n",
    "\n",
    "This will tell us if we have over-trained our network. If testing error increases while training error decreases, then we are over-fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCncRkQKkcBcRKUAKdxGRAqRwFxEpQP8ffuE0LomvEVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ef4dd5a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Show loss curves \n",
    "plt.figure();\n",
    "plt.title('Training performance');\n",
    "plt.plot(history.epoch, history.history['loss'], label='train error');\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='testing error');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Over-fitting\n",
    "\n",
    "Let's go back through the model and add dropout.\n",
    "\n",
    "Here's the idea of what we should see.\n",
    "\n",
    "In the first graph, we have a neural network that it overfit. (The model is not the current PIMA one; instead I am using a CNN for the CIFAR10 image set).\n",
    "\n",
    "![overfitting](overfitting.png)\n",
    "\n",
    "Now I take the same CNN and add in some dropout layers between the convolutional layers. I then re-train the modified model.\n",
    "\n",
    "![overfitting?](overfittingDROPOUT.png)\n",
    "\n",
    "Voila! The training and testing errors now match. So overfitting has been corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
