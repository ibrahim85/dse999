{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Larger CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next line tells Keras that the images will be presented as Tensors with the channels first.\n",
    "\n",
    "Huh? Channels are the red-green-blue channels of the image. So this tells us automatically that CNNs are typically used in image processing neural networks. There are two natural ways of creating Tensors for images. One is to put the channels first (e.g. 3 x 28 x 28). The other is to put the channels last (28 x 28 x 3). TensorFlow does it one way; Theano does it the other. Keras just needs to know which way the raw data is stored so that it can reshape the input Tensors to whichever backend it is using.\n",
    "\n",
    "Note: Our images are greyscale. So the Tensor is really 1 x 28 x 28 (one grey channel from 0 to 255). Also, note that the Tensor is really 4D. The first dimension is the number of images in our dataset. So: # images in data x 1 x 28 x 28. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We normalize the input between 0 and 1\n",
    "\n",
    "So not zero mean, but still normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "We just want to \"one-hot\" encode the output labels. So instead of 0 through 9, we have labels [1,0,0,0,0,0,0,0,0,0] through [0,0,0,0,0,0,0,0,0,1]. So label 5 would be [0,0,0,0,1,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is our actual CNN\n",
    "\n",
    "We first have a 30 neuron CNN layer with a ReLu activation and a 5x5 convolutional filter. Then we do a max pooling on a 2 x 2 window. Then we have a 15 neuron layer with a ReLu activation and a 3 x 3 convolutional filter. Then we do a max pooling of this on a 2x2 window. We also add a 20% dropout to the layer. Then we flatten the image to a single vector (now that we have the spatial convolutional filters). We run that into a 128-neuron layer (ReLu) and a 50-neuron layer (ReLu). The output is the 10-class softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the larger model\n",
    "def larger_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 74s - loss: 0.3990 - acc: 0.8767 - val_loss: 0.0994 - val_acc: 0.9690\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 74s - loss: 0.0971 - acc: 0.9702 - val_loss: 0.0563 - val_acc: 0.9824\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 74s - loss: 0.0695 - acc: 0.9784 - val_loss: 0.0379 - val_acc: 0.9880\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 75s - loss: 0.0567 - acc: 0.9818 - val_loss: 0.0315 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 74s - loss: 0.0477 - acc: 0.9852 - val_loss: 0.0294 - val_acc: 0.9900\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 76s - loss: 0.0435 - acc: 0.9865 - val_loss: 0.0280 - val_acc: 0.9906\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 76s - loss: 0.0386 - acc: 0.9878 - val_loss: 0.0282 - val_acc: 0.9910\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 76s - loss: 0.0343 - acc: 0.9891 - val_loss: 0.0224 - val_acc: 0.9921\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 75s - loss: 0.0323 - acc: 0.9897 - val_loss: 0.0223 - val_acc: 0.9925\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 74s - loss: 0.0281 - acc: 0.9910 - val_loss: 0.0219 - val_acc: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabeef45e90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large CNN Error: 0.70%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# So we've gone from about a 2% error to a less than 1% error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
