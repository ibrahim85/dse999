{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset Keras MLP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Baseline MLP for MNIST dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras has a helper function which will download the MNIST data\n",
    "\n",
    "Let's grab the data and load it in to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First important point!\n",
    "\n",
    "We take the 28x28 image and flatten it to a 1x784 \"image\". This really doesn't make sense, does it? It's an image. So pixels that are close together should be kept close together. That is, there is some natural spatial covariance in pixels-- if pixel A is close in space to pixel B, then pixel A is probably correlated with pixel B. That spatial covariance is lost when we flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "We just want to \"one-hot\" encode the output labels. So instead of 0 through 9, we have labels [1,0,0,0,0,0,0,0,0,0] through [0,0,0,0,0,0,0,0,0,1]. So label 5 would be [0,0,0,0,1,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second important point\n",
    "\n",
    "We just have an input layer that goes to a hidden layer (ReLu activation). Then output layer is a 10 class softmax.\n",
    "\n",
    "This is as simple as a neural network can get, but it does a pretty good job as classifying the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.2793 - acc: 0.9209 - val_loss: 0.1413 - val_acc: 0.9568\n",
      "Epoch 2/10\n",
      "5s - loss: 0.1116 - acc: 0.9676 - val_loss: 0.0919 - val_acc: 0.9714\n",
      "Epoch 3/10\n",
      "5s - loss: 0.0716 - acc: 0.9797 - val_loss: 0.0780 - val_acc: 0.9774\n",
      "Epoch 4/10\n",
      "5s - loss: 0.0503 - acc: 0.9857 - val_loss: 0.0743 - val_acc: 0.9763\n",
      "Epoch 5/10\n",
      "5s - loss: 0.0371 - acc: 0.9894 - val_loss: 0.0686 - val_acc: 0.9791\n",
      "Epoch 6/10\n",
      "5s - loss: 0.0267 - acc: 0.9928 - val_loss: 0.0634 - val_acc: 0.9796\n",
      "Epoch 7/10\n",
      "5s - loss: 0.0207 - acc: 0.9946 - val_loss: 0.0629 - val_acc: 0.9808\n",
      "Epoch 8/10\n",
      "5s - loss: 0.0139 - acc: 0.9968 - val_loss: 0.0639 - val_acc: 0.9804\n",
      "Epoch 9/10\n",
      "5s - loss: 0.0111 - acc: 0.9977 - val_loss: 0.0593 - val_acc: 0.9808\n",
      "Epoch 10/10\n",
      "5s - loss: 0.0077 - acc: 0.9987 - val_loss: 0.0589 - val_acc: 0.9807\n",
      "Baseline Error: 1.93%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
