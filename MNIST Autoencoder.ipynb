{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "An autoencoder is a neural network where the output is supposed to be the same as the input. That's why it is called \"auto\" \"encoder\".\n",
    "\n",
    "![autoencoder](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n",
    "\n",
    "Yep, a neural network that just takes a picture of a cat and outputs the very same picture of a cat. Eee gat! Eee gat! What good is that?\n",
    "\n",
    "Well, it can be very useful.\n",
    "\n",
    "Let's take our standard MNIST dataset. It is a set of 28x28 images-- so 784 dimensions for each image. That's a lot of neurons for our input layer of the neural network. It would be nice if we could reduce the dimensions to make the network training go faster.\n",
    "\n",
    "Principal components analysis is what we've used in the past to reduce the dimensionality. In PCA, we are finding a linear way of reconstructing the images from a smaller set of dimensions (i.e. can 32 dimensions be used to reconstruct the original 784).\n",
    "\n",
    "Autoencoders are a non-linear (and can be linear) way of dimensionality reduction. I should be fair and note that this dimensionality reduction is fairly specific. For example, if I train an autoencoder to reduce images of handwritten numbers (MNIST) there's no reason to think it would work as well on images of cats. So I wouldn't recommend autoencoders as a generic method to use for simply compressing data. But it can be quite good if you are dealing with the same type of data. \n",
    "\n",
    "Hinton has a very good [paper](https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf) showing how deep autoencoders can be used to separate data and make classification easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Let's apply a simple autoencoder to the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data\n",
    "\n",
    "Let's scale the data between 0 and 1. Then we'll unwrap the 28x28 matrix into a 784 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize images between 0 and 1 (rather than 0 to 255)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Reshape the matrix to 1 x 784\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode to a reduce number of dimensions\n",
    "\n",
    "Let's get this down to 32 features from the original 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create autoencoder model\n",
    "\n",
    "So there are 3 layers:\n",
    "1. The input layer that is 784 neurons\n",
    "2. The \"encoding\" layer that is 32 neurons.\n",
    "3. The \"decoding\" layer that is 784 neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "reg = regularizers.l1(10e-7)\n",
    "#reg = None\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu',\n",
    "                 activity_regularizer=reg)(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2970 - val_loss: 0.2045\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1855 - val_loss: 0.1692\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1615 - val_loss: 0.1525\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1481 - val_loss: 0.1416\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1390 - val_loss: 0.1339\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1322 - val_loss: 0.1280\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1268 - val_loss: 0.1234\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1228 - val_loss: 0.1199\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1198 - val_loss: 0.1173\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1176 - val_loss: 0.1155\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1160 - val_loss: 0.1141\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1147 - val_loss: 0.1129\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1137 - val_loss: 0.1120\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1128 - val_loss: 0.1111\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1104\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1097\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1106 - val_loss: 0.1091\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1101 - val_loss: 0.1086\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1081\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1076\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1072\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1069\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1065\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1061\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1058\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1056\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1053\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1050\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1047\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1045\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1043\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1041\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1038\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1037\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1035\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1033\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1033\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1030\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1028\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1026\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1025\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1024\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1022\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1033 - val_loss: 0.1021\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1020\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1030 - val_loss: 0.1019\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1018\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1017\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1016\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1015\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1013\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1012\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1012\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1012\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1010\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1009\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1019 - val_loss: 0.1009\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1008\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1007\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1006\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1005\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1005\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1004\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1014 - val_loss: 0.1003\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1003\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1002\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1001\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1011 - val_loss: 0.1001\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1000\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1010 - val_loss: 0.0999\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1009 - val_loss: 0.0999\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1009 - val_loss: 0.0998\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1008 - val_loss: 0.0997\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1007 - val_loss: 0.0997\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1007 - val_loss: 0.0996\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1006 - val_loss: 0.0995\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1006 - val_loss: 0.0995\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1005 - val_loss: 0.0995\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1005 - val_loss: 0.0995\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1004 - val_loss: 0.0994\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1004 - val_loss: 0.0993\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1003 - val_loss: 0.0993\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1003 - val_loss: 0.0993\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1002 - val_loss: 0.0992\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s - loss: 0.1002 - val_loss: 0.0991\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1001 - val_loss: 0.0990\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1001 - val_loss: 0.0990\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1000 - val_loss: 0.0990\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1000 - val_loss: 0.0990\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.1000 - val_loss: 0.0989\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0999 - val_loss: 0.0989\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0999 - val_loss: 0.0990\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0998 - val_loss: 0.0988\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0998 - val_loss: 0.0988\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0998 - val_loss: 0.0989\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0997 - val_loss: 0.0987\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0997 - val_loss: 0.0986\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0996 - val_loss: 0.0987\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0996 - val_loss: 0.0985\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s - loss: 0.0996 - val_loss: 0.0985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff85de37910>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks = [  # Extra functions called during training\n",
    "                             EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwAAAAAAIAP+P5/df0gOh9u2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff85dd86090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity\n",
    "\n",
    "We can measure the sparsity of the encoder matrix by just getting the mean value. So here we calculate the mean value of the 32 dimensions for all 10,000 test examples. The smaller the number, the more sparse the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69182253,  0.        ,  1.00263691,  0.10057664,  0.85748416,\n",
       "        0.38706851,  0.74713916,  0.28912231,  0.90457207,  0.59719551,\n",
       "        0.28239521,  0.74126256,  0.80670613,  0.60838544,  0.31535879,\n",
       "        0.21921009,  1.14437616,  0.55015767,  0.77234882,  0.73497182,\n",
       "        0.50360519,  0.94996983,  0.89061826,  0.21475574,  1.06839347,\n",
       "        1.40127969,  0.7531997 ,  0.64178181,  0.20284069,  0.47237492,\n",
       "        0.40927809,  0.02662843], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs[24,:]  # Weights for test image 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72388303"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing autoencoders to PCA\n",
    "\n",
    "These are comparisons made in Hinton's [2006 paper](https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf).\n",
    "\n",
    "It's not simply that autoencoders can reduce dimensionality. What Hinton shows is that the dimensions allow different classes to be \"pushed\" apart. It makes classification easier.\n",
    "\n",
    "![mnist](http://nikhilbuduma.com/img/autoencoder_digit_exp.png)\n",
    "Plotting first two dimensions of the MNIST dataset. Left: PCA, Right: Autoencoder\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![gold](http://nikhilbuduma.com/img/autoencoder_nlp_exp.png)\n",
    "Plotting first two dimensions of economic dataset. Left: PCA, Right: Autoencoder\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "![faces](http://nikhilbuduma.com/img/autoencoder_face_exp.png)\n",
    "Top row = Original image,\n",
    "Middle row = 30 dimensional autoencoder reconstruction,\n",
    "Bottom row = 30 dimensional PCA reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
